{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key points\n",
    "\n",
    "-Just extract first 1000-5000 comments for the video "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install emoji\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "\n",
    "#Visualization packages\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add the csv file name in place of UScomments.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comm = pd.read_csv('UScomments.csv',encoding='utf8',error_bad_lines=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For preprocesing, the comments should have a heading as 'comment_text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=comm['comment_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing the emojies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.corpus import stopwords \n",
    "set(stopwords.words('english'))\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer as td\n",
    "import re\n",
    "import emoji\n",
    "\n",
    "def give_emoji_free_text(text):\n",
    "    allchars = [str for str in text]\n",
    "    emoji_list = [c for c in allchars if c in emoji.UNICODE_EMOJI]\n",
    "    clean_text = ' '.join([str for str in text.split() if not any(i in str for i in emoji_list)])\n",
    "    return clean_text\n",
    "for i in range(0,len(df.index)):\n",
    "    df[i]=give_emoji_free_text(df[i])\n",
    "\n",
    "\n",
    "df.head(100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing the punctuations and stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "def word_tokenz(d):\n",
    "    tokenizer=RegexpTokenizer(r'\\w+')\n",
    "    text=tokenizer.tokenize(d)\n",
    "    s=[word for word in text if not word in stopwords.words()] \n",
    "    d=td().detokenize(s)\n",
    "    return d\n",
    "    \n",
    "\n",
    "for i in range(0,len(df.index)):\n",
    "    df[i]=word_tokenz(df[i].lower())\n",
    "    \n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmetization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "for i in range(10000):\n",
    "    sentence=df['comment_text'][i]\n",
    "    try:\n",
    "        words1 = word_tokenize(sentence)\n",
    "    except:\n",
    "        continue\n",
    "    FinalWord=\"\"\n",
    "    for w in words1:\n",
    "        FinalWord+=\" \"+wordnet_lemmatizer.lemmatize(w)   \n",
    "    df['comment_text'][i]=FinalWord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "for i in range(10000):\n",
    "    sentence=df['comment_text'][i]\n",
    "    try:\n",
    "        words2 = word_tokenize(sentence)\n",
    "    except:\n",
    "        continue\n",
    "    FinalWord=\"\"\n",
    "    for w in words2:  \n",
    "        FinalWord+=\" \"+ps.stem(w)\n",
    "    df['comment_text'][i]=FinalWord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# limiting to first 10k entries and droping the null values out of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.head(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
